---
title: r pachong 工作报告
author: jocelyn
date: '2019-05-16'
slug: r-pachong-zhengfu
categories:
  - R
tags: []
---
---
title: 'R  建立博客'
author: jocelyn
date: '2019-05-16'
slug: r
categories:
  - R
tags: []
---
---
title: 'R pachong 政府工作报告'
author: jocelyn
date: '2019-05-16'
slug: r-pachong
categories:
  - R
tags: []
---
---
title: R 爬政府工作报告
author: jocelyn
date: '2019-05-16'
slug: r-1
categories:
  - R
tags: []
---
# 今天我们要用爬虫来爬取这篇文章，然后进行文本分词，再到文本可视化。我们需要使用下面这三个包：

（1）rvest 爬虫

（2）jiebaR 用于分词，统计词频

（3）wordcloud2 用于对文本进行可视化
```{r}
install.packages("rvest")
install.packages("jiebaR")
library("jiebaR")
install.packages("wordcloud2")
library("wordcloud2")
```

# 政府工作报告 http://www.gov.cn/guowuyuan/2019-03/16/content_5374314.htm
打开要爬取的网页，右键查看网页源代码
找到div.pages_content。我们需要爬取的文字就在这个标签里面

%>%管道函数，将左边的值赋给右边函数作为第一个参数的值。

web存储网页信息的变量。

然后html_nodes()函数获取网页里的相应节点。

最后我们用html_text()函数获取标签内的文本信息。

```{r}
library("rvest")
url<-'http://www.gov.cn/guowuyuan/2019-03/16/content_5374314.htm'
web<-read_html(url,encoding="utf-8") #读取数据，规定编码
position<-web %>% html_nodes("div.pages_content") %>% html_text()
print(position)

```
##使用jiebaR进行分词，统计词频

```{r}
library("jiebaR")
engine_s<-worker( )#初始化分词引擎并加载停用词。
seg<-segment(position,engine_s)#分词
f<-freq(seg) #统计词频
f<-f[order(f[2],decreasing=TRUE),] #根据词频降序排列
hist(table(seg))
print(f)
```


### wordcloud 可视化

```{r}

f2<-f[1:150,] #总共有2000多个词，为了显示效果，我只提取前150个字

wordcloud2(f2, size = 1 ,shape='triangle')    #形状设置为三角形

```



























